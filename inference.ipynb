{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query and reArrange \n",
    "\n",
    "Tutorial for Zhao et al., \"Q&A: Query-Based Representation Learning for Multi-Track Symbolic Music re-Arrangement\", accepted by IJCAI 2023 Special Track for AI the Arts and Creativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']= '0'\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from model import Query_and_reArrange\n",
    "from dataset import Slakh2100_Pop909_Dataset, collate_fn_inference, EMBED_PROGRAM_MAPPING\n",
    "SLAKH_CLASS_MAPPING = {v: k for k, v in EMBED_PROGRAM_MAPPING.items()}\n",
    "from utils.format_convert import matrix2midi_with_dynamics, matrix2midi_drum, elem2midi\n",
    "from utils.inferring import mixture_function_prior, search_reference, velocity_adaption\n",
    "import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Symbolic multi-track music rearrangement\n",
    "\n",
    "Based on composition style transfer, Q&A is a generic model for a range of rearrangement problems, including orchestration, piano cover generation, and re-instrumentation. \n",
    "\n",
    "Let's first load the Q&A model. Demo will be saved to `./demo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLAKH2100_DIR = \"./data/Slakh2100\"\n",
    "POP909_DIR = \"./data/Pop909\"\n",
    "MODEL_DIR = \"./checkpoints/Q&A_epoch_029.pt\"\n",
    "SAVE_DIR = './demo'\n",
    "SAMPLE_BAR_LEN = 8\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "model = Query_and_reArrange(name='inference_model', device=DEVICE, trf_layers=2)\n",
    "model.load_state_dict(torch.load(MODEL_DIR))\n",
    "model.to(DEVICE)\n",
    "model.eval();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Orchestration\n",
    "\n",
    "For orchestration, we sample a piano clip $x$ and a multi-track clip $y$, and then orchestrate $x$ using $y$'s track functions (i.e., style).\n",
    "\n",
    "##### 1.1.1 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Pop909 Dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:00<00:00, 118.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Slakh2100 Dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:08<00:00, 26.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering sample space for style references ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20617/20617 [00:52<00:00, 389.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# load piano dataset. A piano piece x is the donor of content.\n",
    "x_set = Slakh2100_Pop909_Dataset(None, POP909_DIR, 16*SAMPLE_BAR_LEN, split='validation', mode='inference', with_dynamics=True)\n",
    "# load multi-track dataset. A multi-track piece y is the donor of style.\n",
    "y_set = Slakh2100_Pop909_Dataset(SLAKH2100_DIR, None, 16*SAMPLE_BAR_LEN, split='validation', mode='inference', with_dynamics=True, with_drums=True)\n",
    "# Prepare for the heuristic sampling of y\n",
    "y_set_loader = DataLoader(y_set, batch_size=1, shuffle=False, collate_fn=lambda b: collate_fn_inference(b, DEVICE))\n",
    "y_prior_set = mixture_function_prior(y_set_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.2 Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to /home/zhaojw/workspace/workspace/Q&A_Rearrangement/repository/demo/orchestration/20230602_144828.\n"
     ]
    }
   ],
   "source": [
    "# get a random x sample\n",
    "IDX = np.random.randint(len(x_set))\n",
    "x = x_set.__getitem__(IDX)\n",
    "(x_mix, x_instr, x_fp, x_ft), (x_dyn, _, _), x_dir = collate_fn_inference(batch = [(x)], device = DEVICE)\n",
    "# heuristic sampling for y (i.e., Equation (8) in the paper)\n",
    "y_anchor = search_reference(x_fp, x_ft, y_prior_set)\n",
    "y = y_set.__getitem__(y_anchor)\n",
    "(y_mix, y_instr, y_fp, y_ft), (y_dyn, y_drm, y_dprog), y_dir = collate_fn_inference(batch=[(y)], device=DEVICE)\n",
    "# exchange x's and y's melody track function in order to preserve the theme melody after rearrangement.\n",
    "x_mel, y_mel = 0, 0\n",
    "y_fp[:, y_mel] = x_fp[:, x_mel]\n",
    "y_ft[:, y_mel] = x_ft[:, x_mel]\n",
    "y_dyn[y_mel] = x_dyn[x_mel]\n",
    "# save x and y\n",
    "save_path = os.path.join(SAVE_DIR, 'orchestration', datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "x_recon = elem2midi(*x, SLAKH_CLASS_MAPPING)\n",
    "x_recon.write(os.path.join(save_path, '01_piano_solo.mid'))\n",
    "y_recon = elem2midi(*y, SLAKH_CLASS_MAPPING)\n",
    "y_recon.write(os.path.join(save_path, '02_reference.mid'))\n",
    "print(f'saved to {save_path}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.3 Calling Q&A for orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to /home/zhaojw/workspace/workspace/Q&A_Rearrangement/repository/demo/orchestration/20230602_144828.\n"
     ]
    }
   ],
   "source": [
    "# Q&A model inference\n",
    "output = model.inference(x_mix, y_instr, y_fp, y_ft, mel_id=y_mel)\n",
    "# apply y's dynamics to the rearrangement result\n",
    "velocity = velocity_adaption(y_dyn[..., 0], output, y_mel)\n",
    "cc = y_dyn[..., 1]\n",
    "output = np.stack([output, velocity, cc], axis=-1)\n",
    "# reconstruct MIDI\n",
    "midi_recon = matrix2midi_with_dynamics(\n",
    "    matrices=output, \n",
    "    programs=[SLAKH_CLASS_MAPPING[item.item()] for item in y_instr[0]], \n",
    "    init_tempo=100)\n",
    "if y_drm is not None:\n",
    "    drum_recon = matrix2midi_drum(y_drm, y_dprog, init_tempo=100)\n",
    "    midi_recon.instruments += drum_recon.instruments\n",
    "midi_recon.write(os.path.join(save_path, '03_orchestration_result.mid'))\n",
    "print(f'saved to {save_path}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Piano cover generation\n",
    "\n",
    "For piano cover generation, we sample a multi-track clip $x$ and a piano clip $y$, and then generate piano cover for $x$ using the textures in $y$.\n",
    "\n",
    "##### 1.2.1 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Slakh2100 Dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:05<00:00, 38.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Pop909 Dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:00<00:00, 155.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering sample space for style references ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6080/6080 [00:10<00:00, 580.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# load piano dataset. A piano piece x is the donor of content.\n",
    "x_set = Slakh2100_Pop909_Dataset(SLAKH2100_DIR, None, 16*SAMPLE_BAR_LEN, split='validation', mode='inference', with_dynamics=True)\n",
    "# load multi-track dataset. A multi-track piece y is the donor of style.\n",
    "y_set = Slakh2100_Pop909_Dataset(None, POP909_DIR, 16*SAMPLE_BAR_LEN, split='validation', mode='inference', with_dynamics=True)\n",
    "# Prepare for the heuristic sampling of y\n",
    "y_set_loader = DataLoader(y_set, batch_size=1, shuffle=False, collate_fn=lambda b: collate_fn_inference(b, DEVICE))\n",
    "y_prior_set = mixture_function_prior(y_set_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2 Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to /home/zhaojw/workspace/workspace/Q&A_Rearrangement/repository/demo/piano_cover/20230602_144919.\n"
     ]
    }
   ],
   "source": [
    "# get a random x sample\n",
    "IDX = np.random.randint(len(x_set))\n",
    "x = x_set.__getitem__(IDX)\n",
    "(x_mix, x_instr, x_fp, x_ft), (x_dyn, _, _), x_dir = collate_fn_inference(batch = [(x)], device = DEVICE)\n",
    "# heuristic sampling for y (i.e., Equation (8) in the paper)\n",
    "y_anchor = search_reference(x_fp, x_ft, y_prior_set)\n",
    "y = y_set.__getitem__(y_anchor)\n",
    "(y_mix, y_instr, y_fp, y_ft), (y_dyn, y_drm, y_dprog), y_dir = collate_fn_inference(batch=[(y)], device=DEVICE)\n",
    "# exchange x's and y's melody track function in order to preserve the theme melody after rearrangement.\n",
    "x_mel, y_mel = 0, 0\n",
    "y_fp[:, y_mel] = x_fp[:, x_mel]\n",
    "y_ft[:, y_mel] = x_ft[:, x_mel]\n",
    "y_dyn[y_mel] = x_dyn[x_mel]\n",
    "# save x and y\n",
    "save_path = os.path.join(SAVE_DIR, 'piano_cover', datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "x_recon = elem2midi(*x, SLAKH_CLASS_MAPPING)\n",
    "x_recon.write(os.path.join(save_path, '01_multi_track.mid'))\n",
    "y_recon = elem2midi(*y, SLAKH_CLASS_MAPPING)\n",
    "y_recon.write(os.path.join(save_path, '02_reference.mid'))\n",
    "print(f'saved to {save_path}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.3 Calling Q&A for piano cover generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to /home/zhaojw/workspace/workspace/Q&A_Rearrangement/repository/demo/piano_cover/20230602_144919.\n"
     ]
    }
   ],
   "source": [
    "# Q&A model inference\n",
    "output = model.inference(x_mix, y_instr, y_fp, y_ft, mel_id=y_mel)\n",
    "# apply y's dynamics to the rearrangement result\n",
    "velocity = velocity_adaption(y_dyn[..., 0], output, y_mel)\n",
    "cc = y_dyn[..., 1]\n",
    "output = np.stack([output, velocity, cc], axis=-1)\n",
    "# reconstruct MIDI\n",
    "midi_recon = matrix2midi_with_dynamics(\n",
    "    matrices=output, \n",
    "    programs=[SLAKH_CLASS_MAPPING[item.item()] for item in y_instr[0]], \n",
    "    init_tempo=100)\n",
    "midi_recon.write(os.path.join(save_path, '03_piano_cover.mid'))\n",
    "print(f'saved to {save_path}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Re-instrumentation\n",
    "\n",
    "For re-instrumentation, we sample multi-track clips $x$ and $y$, and then rearrange $x$ using the track functions in $y$.\n",
    "\n",
    "##### 1.3.1 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Slakh2100 Dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [00:02<00:00, 42.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Slakh2100 Dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:07<00:00, 28.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering sample space for style references ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20617/20617 [00:52<00:00, 389.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# load piano dataset. A piano piece x is the donor of content.\n",
    "x_set = Slakh2100_Pop909_Dataset(SLAKH2100_DIR, None, 16*SAMPLE_BAR_LEN, split='test', mode='inference', with_dynamics=True)\n",
    "# load multi-track dataset. A multi-track piece y is the donor of style.\n",
    "y_set = Slakh2100_Pop909_Dataset(SLAKH2100_DIR, None, 16*SAMPLE_BAR_LEN, split='validation', mode='inference', with_dynamics=True, with_drums=True)\n",
    "# Prepare for the heuristic sampling of y\n",
    "y_set_loader = DataLoader(y_set, batch_size=1, shuffle=False, collate_fn=lambda b: collate_fn_inference(b, DEVICE))\n",
    "y_prior_set = mixture_function_prior(y_set_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3.2 Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to /home/zhaojw/workspace/workspace/Q&A_Rearrangement/repository/demo/re_instrumentation/20230602_145030.\n"
     ]
    }
   ],
   "source": [
    "# get a random x sample\n",
    "IDX = np.random.randint(len(x_set))\n",
    "x = x_set.__getitem__(IDX)\n",
    "(x_mix, x_instr, x_fp, x_ft), (x_dyn, _, _), x_dir = collate_fn_inference(batch = [(x)], device = DEVICE)\n",
    "# heuristic sampling for y (i.e., Equation (8) in the paper)\n",
    "y_anchor = search_reference(x_fp, x_ft, y_prior_set)\n",
    "y = y_set.__getitem__(y_anchor)\n",
    "(y_mix, y_instr, y_fp, y_ft), (y_dyn, y_drm, y_dprog), y_dir = collate_fn_inference(batch=[(y)], device=DEVICE)\n",
    "# exchange x's and y's melody track function in order to preserve the theme melody after rearrangement.\n",
    "x_mel, y_mel = 0, 0\n",
    "y_fp[:, y_mel] = x_fp[:, x_mel]\n",
    "y_ft[:, y_mel] = x_ft[:, x_mel]\n",
    "y_dyn[y_mel] = x_dyn[x_mel]\n",
    "# save x and y\n",
    "save_path = os.path.join(SAVE_DIR, 're_instrumentation', datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "x_recon = elem2midi(*x, SLAKH_CLASS_MAPPING)\n",
    "x_recon.write(os.path.join(save_path, '01_multi_track.mid'))\n",
    "y_recon = elem2midi(*y, SLAKH_CLASS_MAPPING)\n",
    "y_recon.write(os.path.join(save_path, '02_reference.mid'))\n",
    "print(f'saved to {save_path}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3.3 Calling Q&A for re-instrumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to /home/zhaojw/workspace/workspace/Q&A_Rearrangement/repository/demo/re_instrumentation/20230602_145030.\n"
     ]
    }
   ],
   "source": [
    "# Q&A model inference\n",
    "output = model.inference(x_mix, y_instr, y_fp, y_ft, mel_id=y_mel)\n",
    "# apply y's dynamics to the rearrangement result\n",
    "velocity = velocity_adaption(y_dyn[..., 0], output, y_mel)\n",
    "cc = y_dyn[..., 1]\n",
    "output = np.stack([output, velocity, cc], axis=-1)\n",
    "# reconstruct MIDI\n",
    "midi_recon = matrix2midi_with_dynamics(\n",
    "    matrices=output, \n",
    "    programs=[SLAKH_CLASS_MAPPING[item.item()] for item in y_instr[0]], \n",
    "    init_tempo=100)\n",
    "if y_drm is not None:\n",
    "    drum_recon = matrix2midi_drum(y_drm, y_dprog, 100)\n",
    "    midi_recon.instruments += drum_recon.instruments\n",
    "midi_recon.write(os.path.join(save_path, '03_re_instrumentation.mid'))\n",
    "print(f'saved to {save_path}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Voice separation\n",
    "\n",
    "By inferring track functions as voice hints, Q&A can additionally handle voice separation.\n",
    "\n",
    "Let's load Q&A-V, our variant model for voice separation. Demo will be saved to `./demo/voice_separation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Query_and_reArrange_vocie_separation\n",
    "from dataset import Voice_Separation_Dataset\n",
    "from utils.format_convert import matrix2midi, mixture2midi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bach chorales\n",
    "\n",
    "Four-voice separation on Bach chorales.\n",
    "\n",
    "##### 2.1.1 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Bach Chorale Dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:00<00:00, 257.39it/s]\n"
     ]
    }
   ],
   "source": [
    "BACH_DIR = \"./data/Chorales\"\n",
    "QUARTETS_DIR = None\n",
    "MODEL_DIR = \"./checkpoints/Q&A_chorales_epoch_041.pt\"\n",
    "SAVE_DIR = './demo'\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "model = Query_and_reArrange_vocie_separation(name='inference_model', device=DEVICE, trf_layers=2)\n",
    "model.load_state_dict(torch.load(MODEL_DIR))\n",
    "model.to(DEVICE)\n",
    "model.eval();\n",
    "\n",
    "x_set = Voice_Separation_Dataset(BACH_DIR, QUARTETS_DIR, 'full', split='validation', mode='inference')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.2 Sampling a mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to /home/zhaojw/workspace/workspace/Q&A_Rearrangement/repository/demo/voice_separation/20230602_145056.\n"
     ]
    }
   ],
   "source": [
    "# get a random x sample\n",
    "IDX = np.random.randint(len(x_set))\n",
    "x = x_set.__getitem__(IDX)\n",
    "(x_mix, x_instr, _, _), (_, _, _), x_dir = collate_fn_inference(batch = [(x)], device = DEVICE)\n",
    "# save mixture\n",
    "save_path = os.path.join(SAVE_DIR, 'voice_separation', datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "x_recon = mixture2midi(x_mix)\n",
    "x_recon.write(os.path.join(save_path, f\"01_mixture_{x_dir.replace('.npz', '')}.mid\"))\n",
    "print(f'saved to {save_path}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.3 Calling Q&A for Bach chorales voice separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to /home/zhaojw/workspace/workspace/Q&A_Rearrangement/repository/demo/voice_separation/20230602_145056.\n"
     ]
    }
   ],
   "source": [
    "output = model.inference(x_mix, x_instr)\n",
    "midi_recon = matrix2midi(output, programs=[52]*4, init_tempo=100)\n",
    "midi_recon.write(os.path.join(save_path, '02_voice_separation.mid'))\n",
    "print(f'saved to {save_path}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. String quartets\n",
    "\n",
    "Four-voice separation on string quartets.\n",
    "\n",
    "##### 2.2.1 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading String Quartets Dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 34.85it/s]\n"
     ]
    }
   ],
   "source": [
    "BACH_DIR = None\n",
    "QUARTETS_DIR = './data/Quartets'\n",
    "MODEL_DIR = \"./checkpoints/Q&A_quartets_epoch_029.pt\"\n",
    "SAVE_DIR = './demo'\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "model = Query_and_reArrange_vocie_separation(name='inference_model', device=DEVICE, trf_layers=2)\n",
    "model.load_state_dict(torch.load(MODEL_DIR))\n",
    "model.to(DEVICE)\n",
    "model.eval();\n",
    "\n",
    "x_set = Voice_Separation_Dataset(BACH_DIR, QUARTETS_DIR, 'full', split='validation', mode='inference')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 Sampling a mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to /home/zhaojw/workspace/workspace/Q&A_Rearrangement/repository/demo/voice_separation/20230602_145104.\n"
     ]
    }
   ],
   "source": [
    "# get a random x sample\n",
    "IDX = np.random.randint(len(x_set))\n",
    "x = x_set.__getitem__(IDX)\n",
    "(x_mix, x_instr, _, _), (_, _, _), x_dir = collate_fn_inference(batch = [(x)], device = DEVICE)\n",
    "# save mixture\n",
    "save_path = os.path.join(SAVE_DIR, 'voice_separation', datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "x_recon = mixture2midi(x_mix)\n",
    "x_recon.write(os.path.join(save_path, f\"01_mixture_{x_dir.replace('.npz', '')}.mid\"))\n",
    "print(f'saved to {save_path}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.3 Calling Q&A for string quartets voice separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to /home/zhaojw/workspace/workspace/Q&A_Rearrangement/repository/demo/voice_separation/20230602_145104.\n"
     ]
    }
   ],
   "source": [
    "output = model.inference(x_mix, x_instr)\n",
    "midi_recon = matrix2midi(output, programs=[40, 40, 41, 42], init_tempo=100)\n",
    "midi_recon.write(os.path.join(save_path, '02_voice_separation.mid'))\n",
    "print(f'saved to {save_path}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.10_conda11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
