{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query-and-reArrange\n",
    "\n",
    "* Tutorial notebook for Zhao et al., [Q&A: Query-Based Representation Learning for Multi-Track Symbolic Music re-Arrangement](https://arxiv.org/abs/2306.01635), accepted by IJCAI 2023 Special Track for AI the Arts and Creativity.\n",
    "\n",
    "* Based on composition style transfer, Q&A is a generic model for a range of symbolic rearrangement problems, including 1) **orchestration**, 2) **piano cover generation**, 3) **re-instrumentation**, and 4) **voice separation**. We will demonstrate each case in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']= '0'\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from model import Query_and_reArrange\n",
    "from dataset import Slakh2100_Pop909_Dataset, collate_fn_inference, EMBED_PROGRAM_MAPPING\n",
    "SLAKH_CLASS_MAPPING = {v: k for k, v in EMBED_PROGRAM_MAPPING.items()}\n",
    "from utils.format_convert import matrix2midi_with_dynamics, dataitem2midi\n",
    "from utils.inferring import mixture_function_prior, search_reference, velocity_adaption\n",
    "import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Symbolic multi-track music rearrangement\n",
    "\n",
    "* In the following, we apply Q&A to perform **rearrangement** on $8$-bar music samples (i.e., `SAMPLE_BAR_LEN`=$8$). Demo will be saved to `./demo`.\n",
    "\n",
    "* We first sample a *source* piece $x$ as the donor of content, and then sample a *reference* piece $y$ as the donor of track functions (style). Later, we apply Q&A to generate *target* piece $\\hat{x}$, which is the rearrangement version of $x$ using the style of $y$.\n",
    "\n",
    "* We set `DEBUG_MODE`=`True` and load a small portion of the sample datasets. You may toggle this setting if you have a sufficient RAM and more diverse results will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "POP909_DIR = \"./data/POP909\"\n",
    "SLAKH2100_DIR = \"./data/Slakh2100\"\n",
    "with open(\"./data/slakh_melody_check.json\", 'r') as f:\n",
    "    MEL_CHECK = json.load(f)\n",
    "SAVE_DIR = './demo'\n",
    "\n",
    "SAMPLE_BAR_LEN = 8\n",
    "\n",
    "MODEL_DIR = \"./checkpoints/Q&A_epoch_029.pt\"\n",
    "DEVICE = 'cuda:0'\n",
    "model = Query_and_reArrange(name='inference_model', device=DEVICE, trf_layers=2)\n",
    "model.load_state_dict(torch.load(MODEL_DIR, map_location='cpu'))\n",
    "model.to(DEVICE)\n",
    "model.eval();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Orchestration\n",
    "\n",
    "* For orchestration, we sample a piano clip $x$ from POP909 and a multi-track clip $y$ from Slakh2100, and then orchestrate $x$ using $y$'s style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Pop909 Dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 63.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Slakh2100 Dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 22.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering sample space for style references ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 857/857 [00:03<00:00, 240.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# load piano dataset. A piano piece x is the donor of content.\n",
    "x_set = Slakh2100_Pop909_Dataset(None, POP909_DIR, 16*SAMPLE_BAR_LEN, debug_mode=True, split='validation', mode='inference', with_dynamics=True)\n",
    "# load multi-track dataset. A multi-track piece y is the donor of style.\n",
    "y_set = Slakh2100_Pop909_Dataset(SLAKH2100_DIR, None, 16*SAMPLE_BAR_LEN, debug_mode=True, split='validation', mode='inference', with_dynamics=True)\n",
    "# Prepare for the heuristic sampling of y\n",
    "y_set_loader = DataLoader(y_set, batch_size=1, shuffle=False, collate_fn=lambda b: collate_fn_inference(b, DEVICE))\n",
    "y_prior_set = mixture_function_prior(y_set_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sampling source piece $x$ from POP909"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to ./demo\\orchestration-230604164637.\n"
     ]
    }
   ],
   "source": [
    "# get a random x sample\n",
    "IDX = np.random.randint(len(x_set))\n",
    "x = x_set.__getitem__(IDX)\n",
    "(x_mix, x_instr, x_fp, x_ft), x_dyn, x_dir = collate_fn_inference(batch = [(x)], device = DEVICE)\n",
    "# save x\n",
    "save_path = os.path.join(SAVE_DIR, f\"orchestration-{datetime.datetime.now().strftime('%Y%m%d%H%M%S')[2:]}\")\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "x_recon = dataitem2midi(*x, SLAKH_CLASS_MAPPING)\n",
    "x_recon.write(os.path.join(save_path, '01_source.mid'))\n",
    "print(f'saved to {save_path}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calling Q&A for **orchestration** after sampling reference piece $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to ./demo\\orchestration-230604164637.\n"
     ]
    }
   ],
   "source": [
    "# heuristic sampling for y (i.e., Equation (8) in the paper)\n",
    "y_anchor = search_reference(x_fp, x_ft, y_prior_set)\n",
    "y = y_set.__getitem__(y_anchor)\n",
    "(y_mix, y_instr, y_fp, y_ft), y_dyn, y_dir = collate_fn_inference(batch=[(y)], device=DEVICE)\n",
    "# exchange x's and y's melody track function in order to preserve the theme melody after rearrangement.\n",
    "x_mel, y_mel = 0, MEL_CHECK[y_dir.replace('\\\\', '/').split('/')[-1].replace('.npz', '')]\n",
    "y_fp[:, y_mel] = x_fp[:, x_mel]\n",
    "y_ft[:, y_mel] = x_ft[:, x_mel]\n",
    "#save y\n",
    "y_recon = dataitem2midi(*y, SLAKH_CLASS_MAPPING)\n",
    "y_recon.write(os.path.join(save_path, '02_reference.mid'))\n",
    "\n",
    "# Q&A model inference\n",
    "output = model.inference(x_mix, y_instr, y_fp, y_ft, mel_id=y_mel)\n",
    "# apply y's dynamics to the rearrangement result\n",
    "velocity = velocity_adaption(y_dyn[..., 0], output, y_mel)\n",
    "cc = y_dyn[..., 1]\n",
    "output = np.stack([output, velocity, cc], axis=-1)\n",
    "# reconstruct MIDI\n",
    "midi_recon = matrix2midi_with_dynamics(\n",
    "    matrices=output, \n",
    "    programs=[SLAKH_CLASS_MAPPING[item.item()] for item in y_instr[0]], \n",
    "    init_tempo=100)\n",
    "midi_recon.write(os.path.join(save_path, '03_target.mid'))\n",
    "print(f'saved to {save_path}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Piano Cover Generation\n",
    "\n",
    "* For piano cover generation, we sample a multi-track clip $x$ from Slakh2100 and a piano clip $y$ from POP909, and then rearrange $x$ using $y$'s textures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Slakh2100 Dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 20.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Pop909 Dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 59.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering sample space for style references ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 727/727 [00:02<00:00, 267.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# load piano dataset. A piano piece x is the donor of content.\n",
    "x_set = Slakh2100_Pop909_Dataset(SLAKH2100_DIR, None, 16*SAMPLE_BAR_LEN, debug_mode=True, split='validation', mode='inference', with_dynamics=True)\n",
    "# load multi-track dataset. A multi-track piece y is the donor of style.\n",
    "y_set = Slakh2100_Pop909_Dataset(None, POP909_DIR, 16*SAMPLE_BAR_LEN, debug_mode=True, split='validation', mode='inference', with_dynamics=True)\n",
    "# Prepare for the heuristic sampling of y\n",
    "y_set_loader = DataLoader(y_set, batch_size=1, shuffle=False, collate_fn=lambda b: collate_fn_inference(b, DEVICE))\n",
    "y_prior_set = mixture_function_prior(y_set_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sampling source piece $x$ from Slakh2100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to ./demo\\pianocover-230604164651.\n"
     ]
    }
   ],
   "source": [
    "# get a random x sample\n",
    "IDX = np.random.randint(len(x_set))\n",
    "x = x_set.__getitem__(IDX)\n",
    "(x_mix, x_instr, x_fp, x_ft), x_dyn, x_dir = collate_fn_inference(batch = [(x)], device = DEVICE)\n",
    "# save x\n",
    "save_path = os.path.join(SAVE_DIR, f\"pianocover-{datetime.datetime.now().strftime('%Y%m%d%H%M%S')[2:]}\")\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "x_recon = dataitem2midi(*x, SLAKH_CLASS_MAPPING)\n",
    "x_recon.write(os.path.join(save_path, '01_source.mid'))\n",
    "print(f'saved to {save_path}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calling Q&A for **piano cover generation** after sampling $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to ./demo\\pianocover-230604164651.\n"
     ]
    }
   ],
   "source": [
    "# heuristic sampling for y (i.e., Equation (8) in the paper)\n",
    "y_anchor = search_reference(x_fp, x_ft, y_prior_set)\n",
    "y = y_set.__getitem__(y_anchor)\n",
    "(y_mix, y_instr, y_fp, y_ft), y_dyn, y_dir = collate_fn_inference(batch=[(y)], device=DEVICE)\n",
    "# exchange x's and y's melody track function in order to preserve the theme melody after rearrangement.\n",
    "x_mel, y_mel = MEL_CHECK[x_dir.replace('\\\\', '/').split('/')[-1].replace('.npz', '')], 0\n",
    "y_fp[:, y_mel] = x_fp[:, x_mel]\n",
    "y_ft[:, y_mel] = x_ft[:, x_mel]\n",
    "# save y\n",
    "y_recon = dataitem2midi(*y, SLAKH_CLASS_MAPPING)\n",
    "y_recon.write(os.path.join(save_path, '02_reference.mid'))\n",
    "\n",
    "# Q&A model inference\n",
    "output = model.inference(x_mix, y_instr, y_fp, y_ft, mel_id=y_mel)\n",
    "# apply y's dynamics to the rearrangement result\n",
    "velocity = velocity_adaption(y_dyn[..., 0], output, y_mel)\n",
    "cc = y_dyn[..., 1]\n",
    "output = np.stack([output, velocity, cc], axis=-1)\n",
    "# reconstruct MIDI\n",
    "midi_recon = matrix2midi_with_dynamics(\n",
    "    matrices=output, \n",
    "    programs=[SLAKH_CLASS_MAPPING[item.item()] for item in y_instr[0]], \n",
    "    init_tempo=100)\n",
    "midi_recon.write(os.path.join(save_path, '03_target.mid'))\n",
    "print(f'saved to {save_path}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Re-Instrumentation\n",
    "\n",
    "* For re-instrumentation, we sample multi-track clips $x$ and $y$ both from Slakh2100, and then rearrange $x$ using $y$'s style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Slakh2100 Dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Slakh2100 Dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 18.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering sample space for style references ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 857/857 [00:04<00:00, 186.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# load piano dataset. A piano piece x is the donor of content.\n",
    "x_set = Slakh2100_Pop909_Dataset(SLAKH2100_DIR, None, 16*SAMPLE_BAR_LEN, debug_mode=True, split='test', mode='inference', with_dynamics=True)\n",
    "# load multi-track dataset. A multi-track piece y is the donor of style.\n",
    "y_set = Slakh2100_Pop909_Dataset(SLAKH2100_DIR, None, 16*SAMPLE_BAR_LEN, debug_mode=True, split='validation', mode='inference', with_dynamics=True)\n",
    "# Prepare for the heuristic sampling of y\n",
    "y_set_loader = DataLoader(y_set, batch_size=1, shuffle=False, collate_fn=lambda b: collate_fn_inference(b, DEVICE))\n",
    "y_prior_set = mixture_function_prior(y_set_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sampling source piece $x$ from Slakh2100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to ./demo\\reinstrumentation-230604164702.\n"
     ]
    }
   ],
   "source": [
    "# get a random x sample\n",
    "IDX = np.random.randint(len(x_set))\n",
    "x = x_set.__getitem__(IDX)\n",
    "(x_mix, x_instr, x_fp, x_ft), x_dyn, x_dir = collate_fn_inference(batch = [(x)], device = DEVICE)\n",
    "# save x\n",
    "save_path = os.path.join(SAVE_DIR, f\"reinstrumentation-{datetime.datetime.now().strftime('%Y%m%d%H%M%S')[2:]}\")\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "x_recon = dataitem2midi(*x, SLAKH_CLASS_MAPPING)\n",
    "x_recon.write(os.path.join(save_path, '01_source.mid'))\n",
    "print(f'saved to {save_path}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calling Q&A for **re-instrumentation** after sampling $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to ./demo\\reinstrumentation-230604164702.\n"
     ]
    }
   ],
   "source": [
    "# heuristic sampling for y (i.e., Equation (8) in the paper)\n",
    "y_anchor = search_reference(x_fp, x_ft, y_prior_set)\n",
    "y = y_set.__getitem__(y_anchor)\n",
    "(y_mix, y_instr, y_fp, y_ft), y_dyn, y_dir = collate_fn_inference(batch=[(y)], device=DEVICE)\n",
    "# exchange x's and y's melody track function in order to preserve the theme melody after rearrangement.\n",
    "x_mel, y_mel = MEL_CHECK[x_dir.replace('\\\\', '/').split('/')[-1].replace('.npz', '')], MEL_CHECK[y_dir.replace('\\\\', '/').split('/')[-1].replace('.npz', '')]\n",
    "y_fp[:, y_mel] = x_fp[:, x_mel]\n",
    "y_ft[:, y_mel] = x_ft[:, x_mel]\n",
    "# save y\n",
    "y_recon = dataitem2midi(*y, SLAKH_CLASS_MAPPING)\n",
    "y_recon.write(os.path.join(save_path, '02_reference.mid'))\n",
    "\n",
    "# Q&A model inference\n",
    "output = model.inference(x_mix, y_instr, y_fp, y_ft, mel_id=y_mel)\n",
    "# apply y's dynamics to the rearrangement result\n",
    "velocity = velocity_adaption(y_dyn[..., 0], output, y_mel)\n",
    "cc = y_dyn[..., 1]\n",
    "output = np.stack([output, velocity, cc], axis=-1)\n",
    "# reconstruct MIDI\n",
    "midi_recon = matrix2midi_with_dynamics(\n",
    "    matrices=output, \n",
    "    programs=[SLAKH_CLASS_MAPPING[item.item()] for item in y_instr[0]], \n",
    "    init_tempo=100)\n",
    "midi_recon.write(os.path.join(save_path, '03_target.mid'))\n",
    "print(f'saved to {save_path}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Voice separation\n",
    "\n",
    "* By inferring track functions as voice hints, Q&A can additionally handle voice separation.\n",
    "* We assume a preset total number of voices, which equals to 4 in our case.\n",
    "* In the following, we apply Q&A for voice separation on Bach chorales and string quartets. Demo will be saved to `./demo/voice_separation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Query_and_reArrange_vocie_separation\n",
    "from dataset import Voice_Separation_Dataset\n",
    "from utils.format_convert import matrix2midi, mixture2midi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bach chorales\n",
    "\n",
    "* Loading Bach Chorales dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Bach Chorale Dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:00<00:00, 99.02it/s]\n"
     ]
    }
   ],
   "source": [
    "BACH_DIR = \"./data/Bach_Chorales\"\n",
    "QUARTETS_DIR = None\n",
    "SAVE_DIR = './demo'\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "MODEL_DIR = \"./checkpoints/Q&A_chorales_epoch_041.pt\"\n",
    "model = Query_and_reArrange_vocie_separation(name='inference_model', device=DEVICE, trf_layers=2)\n",
    "model.load_state_dict(torch.load(MODEL_DIR))\n",
    "model.to(DEVICE)\n",
    "model.eval();\n",
    "\n",
    "x_set = Voice_Separation_Dataset(BACH_DIR, QUARTETS_DIR, 'full', split='validation', mode='inference')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sample a mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to ./demo\\voiceseparation-bwv430-230604164718.\n"
     ]
    }
   ],
   "source": [
    "# get a random x sample\n",
    "IDX = np.random.randint(len(x_set))\n",
    "x = x_set.__getitem__(IDX)\n",
    "(x_mix, x_instr, _, _), _, x_dir = collate_fn_inference(batch = [(x)], device = DEVICE)\n",
    "# save mixture\n",
    "save_path = os.path.join(SAVE_DIR, f\"voiceseparation-{x_dir.replace('.npz', '')}-{datetime.datetime.now().strftime('%Y%m%d%H%M%S')[2:]}\")\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "x_recon = mixture2midi(x_mix)\n",
    "x_recon.write(os.path.join(save_path, f\"01_source.mid\"))\n",
    "print(f'saved to {save_path}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calling Q&A for **voice separation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to ./demo\\voiceseparation-bwv430-230604164718.\n"
     ]
    }
   ],
   "source": [
    "output = model.inference(x_mix, x_instr)\n",
    "midi_recon = matrix2midi(output, programs=[52]*4, init_tempo=100)\n",
    "midi_recon.write(os.path.join(save_path, '02_target.mid'))\n",
    "print(f'saved to {save_path}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 String Quartets\n",
    "\n",
    "* Loading String Quartets dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading String Quartets Dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 33.86it/s]\n"
     ]
    }
   ],
   "source": [
    "BACH_DIR = None\n",
    "QUARTETS_DIR = './data/String_Quartets'\n",
    "MODEL_DIR = \"./checkpoints/Q&A_quartets_epoch_029.pt\"\n",
    "SAVE_DIR = './demo'\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "model = Query_and_reArrange_vocie_separation(name='inference_model', device=DEVICE, trf_layers=2)\n",
    "model.load_state_dict(torch.load(MODEL_DIR))\n",
    "model.to(DEVICE)\n",
    "model.eval();\n",
    "\n",
    "x_set = Voice_Separation_Dataset(BACH_DIR, QUARTETS_DIR, 'full', split='validation', mode='inference')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sample a mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to ./demo\\voiceseparation-Ravel-2180_gr_rqtf4-230604164725.\n"
     ]
    }
   ],
   "source": [
    "# get a random x sample\n",
    "IDX = np.random.randint(len(x_set))\n",
    "x = x_set.__getitem__(IDX)\n",
    "(x_mix, x_instr, _, _), _, x_dir = collate_fn_inference(batch = [(x)], device = DEVICE)\n",
    "# save mixture\n",
    "save_path = os.path.join(SAVE_DIR, f\"voiceseparation-{x_dir.replace('.npz', '')}-{datetime.datetime.now().strftime('%Y%m%d%H%M%S')[2:]}\")\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "x_recon = mixture2midi(x_mix)\n",
    "x_recon.write(os.path.join(save_path, f\"01_source.mid\"))\n",
    "print(f'saved to {save_path}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calling Q&A for **voice separation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to ./demo\\voiceseparation-Ravel-2180_gr_rqtf4-230604164725.\n"
     ]
    }
   ],
   "source": [
    "output = model.inference(x_mix, x_instr)\n",
    "midi_recon = matrix2midi(output, programs=[40, 40, 41, 42], init_tempo=100)\n",
    "midi_recon.write(os.path.join(save_path, '02_target.mid'))\n",
    "print(f'saved to {save_path}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.10_conda11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
